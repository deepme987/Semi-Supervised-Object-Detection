/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO - 04/29/22 14:52:27 - 0:00:00 - ============ Initialized logger ============
INFO - 04/29/22 14:52:27 - 0:00:00 - arch: resnet50
                                     base_lr: 0.5
                                     batch_size: 128
                                     checkpoint_freq: 5
                                     crops_for_assign: [0, 1]
                                     data_path: /unlabeled
                                     dist_url: env://
                                     dump_checkpoints: ./checkpoints
                                     dump_path: .
                                     epoch_queue_starts: 12
                                     epochs: 70
                                     epsilon: 0.05
                                     feat_dim: 128
                                     final_lr: 0.0005
                                     freeze_prototypes_niters: 5005
                                     hidden_mlp: 2048
                                     local_rank: 3
                                     max_scale_crops: [1.0, 0.14]
                                     min_scale_crops: [0.14, 0.05]
                                     nmb_crops: [2, 6]
                                     nmb_prototypes: 2000
                                     queue_length: 2560
                                     rank: 3
                                     seed: 31
                                     sinkhorn_iterations: 3
                                     size_crops: [224, 96]
                                     start_warmup: 0
                                     syncbn_process_group_size: 2
                                     temperature: 0.1
                                     trial: -1
                                     use_fp16: False
                                     warmup_epochs: 0
                                     wd: 0.0001
                                     workers: 8
                                     world_size: 4
INFO - 04/29/22 14:52:27 - 0:00:00 - The experiment will be stored in .
                                     

INFO - 04/29/22 14:52:27 - 0:00:00 - ============ Initialized logger ============
INFO - 04/29/22 14:52:27 - 0:00:00 - arch: resnet50
                                     base_lr: 0.5
                                     batch_size: 128
                                     checkpoint_freq: 5
                                     crops_for_assign: [0, 1]
                                     data_path: /unlabeled
                                     dist_url: env://
                                     dump_checkpoints: ./checkpoints
                                     dump_path: .
                                     epoch_queue_starts: 12
                                     epochs: 70
                                     epsilon: 0.05
                                     feat_dim: 128
                                     final_lr: 0.0005
                                     freeze_prototypes_niters: 5005
                                     hidden_mlp: 2048
                                     local_rank: 2
                                     max_scale_crops: [1.0, 0.14]
                                     min_scale_crops: [0.14, 0.05]
                                     nmb_crops: [2, 6]
                                     nmb_prototypes: 2000
                                     queue_length: 2560
                                     rank: 2
                                     seed: 31
                                     sinkhorn_iterations: 3
                                     size_crops: [224, 96]
                                     start_warmup: 0
                                     syncbn_process_group_size: 2
                                     temperature: 0.1
                                     trial: -1
                                     use_fp16: False
                                     warmup_epochs: 0
                                     wd: 0.0001
                                     workers: 8
                                     world_size: 4
INFO - 04/29/22 14:52:27 - 0:00:00 - The experiment will be stored in .
                                     

INFO - 04/29/22 14:52:27 - 0:00:00 - ============ Initialized logger ============
INFO - 04/29/22 14:52:27 - 0:00:00 - arch: resnet50
                                     base_lr: 0.5
                                     batch_size: 128
                                     checkpoint_freq: 5
                                     crops_for_assign: [0, 1]
                                     data_path: /unlabeled
                                     dist_url: env://
                                     dump_checkpoints: ./checkpoints
                                     dump_path: .
                                     epoch_queue_starts: 12
                                     epochs: 70
                                     epsilon: 0.05
                                     feat_dim: 128
                                     final_lr: 0.0005
                                     freeze_prototypes_niters: 5005
                                     hidden_mlp: 2048
                                     local_rank: 0
                                     max_scale_crops: [1.0, 0.14]
                                     min_scale_crops: [0.14, 0.05]
                                     nmb_crops: [2, 6]
                                     nmb_prototypes: 2000
                                     queue_length: 2560
                                     rank: 0
                                     seed: 31
                                     sinkhorn_iterations: 3
                                     size_crops: [224, 96]
                                     start_warmup: 0
                                     syncbn_process_group_size: 2
                                     temperature: 0.1
                                     trial: -1
                                     use_fp16: False
                                     warmup_epochs: 0
                                     wd: 0.0001
                                     workers: 8
                                     world_size: 4
INFO - 04/29/22 14:52:27 - 0:00:00 - The experiment will be stored in .
                                     

INFO - 04/29/22 14:52:27 - 0:00:00 - ============ Initialized logger ============
INFO - 04/29/22 14:52:27 - 0:00:00 - arch: resnet50
                                     base_lr: 0.5
                                     batch_size: 128
                                     checkpoint_freq: 5
                                     crops_for_assign: [0, 1]
                                     data_path: /unlabeled
                                     dist_url: env://
                                     dump_checkpoints: ./checkpoints
                                     dump_path: .
                                     epoch_queue_starts: 12
                                     epochs: 70
                                     epsilon: 0.05
                                     feat_dim: 128
                                     final_lr: 0.0005
                                     freeze_prototypes_niters: 5005
                                     hidden_mlp: 2048
                                     local_rank: 1
                                     max_scale_crops: [1.0, 0.14]
                                     min_scale_crops: [0.14, 0.05]
                                     nmb_crops: [2, 6]
                                     nmb_prototypes: 2000
                                     queue_length: 2560
                                     rank: 1
                                     seed: 31
                                     sinkhorn_iterations: 3
                                     size_crops: [224, 96]
                                     start_warmup: 0
                                     syncbn_process_group_size: 2
                                     temperature: 0.1
                                     trial: -1
                                     use_fp16: False
                                     warmup_epochs: 0
                                     wd: 0.0001
                                     workers: 8
                                     world_size: 4
INFO - 04/29/22 14:52:27 - 0:00:00 - The experiment will be stored in .
                                     

INFO - 04/29/22 14:52:27 - 0:00:00 - Using 4 GPUs.
INFO - 04/29/22 14:52:27 - 0:00:00 - Using 4 GPUs.
INFO - 04/29/22 14:52:27 - 0:00:00 - Using 4 GPUs.
INFO - 04/29/22 14:52:27 - 0:00:00 - Using 4 GPUs.
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #0: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #1: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #2: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #3: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #0: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #1: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #2: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #3: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #0: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #1: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #2: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #3: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #0: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #1: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #2: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:27 - 0:00:00 - Device #3: Tesla V100-SXM2-16GB
INFO - 04/29/22 14:52:28 - 0:00:01 - Building data done with 511997 images loaded.
INFO - 04/29/22 14:52:28 - 0:00:01 - Building data done with 511997 images loaded.
INFO - 04/29/22 14:52:28 - 0:00:01 - Building data done with 511997 images loaded.
INFO - 04/29/22 14:52:28 - 0:00:01 - Building data done with 511997 images loaded.
INFO - 04/29/22 14:52:46 - 0:00:19 - Building model done.
INFO - 04/29/22 14:52:46 - 0:00:19 - Building model done.
INFO - 04/29/22 14:52:46 - 0:00:19 - Building model done.
INFO - 04/29/22 14:52:46 - 0:00:19 - MyDataParallel(
                                       (module): ResNet(
                                         (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)
                                         (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)
                                         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                         (relu): ReLU(inplace=True)
                                         (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
                                         (layer1): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                               (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (layer2): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                               (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (3): Bottleneck(
                                             (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (layer3): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                               (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (3): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (4): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (5): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (layer4): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                               (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                                         (projection_head): Sequential(
                                           (0): Linear(in_features=2048, out_features=2048, bias=True)
                                           (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                           (2): ReLU(inplace=True)
                                           (3): Linear(in_features=2048, out_features=128, bias=True)
                                         )
                                         (prototypes): Linear(in_features=128, out_features=2000, bias=False)
                                       )
                                     )
INFO - 04/29/22 14:52:46 - 0:00:19 - Building model done.
INFO - 04/29/22 14:52:47 - 0:00:20 - Building optimizer done.
INFO - 04/29/22 14:52:47 - 0:00:20 - ============ Starting epoch 0 ... ============
INFO - 04/29/22 14:52:47 - 0:00:20 - Building optimizer done.
INFO - 04/29/22 14:52:47 - 0:00:20 - ============ Starting epoch 0 ... ============
INFO - 04/29/22 14:52:47 - 0:00:20 - Building optimizer done.
INFO - 04/29/22 14:52:47 - 0:00:20 - ============ Starting epoch 0 ... ============
INFO - 04/29/22 14:52:47 - 0:00:20 - Building optimizer done.
INFO - 04/29/22 14:52:47 - 0:00:20 - ============ Starting epoch 0 ... ============
Traceback (most recent call last):
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 408, in <module>
    main()
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 254, in main
    scores, queue = train(train_loader, model, optimizer, epoch, lr_schedule, queue)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 312, in train
    embedding, output = model(inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 317, in forward
    _out = self.forward_backbone(torch.cat(inputs[start_idx: end_idx]).cuda(non_blocking=True))
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 285, in forward_backbone
    x = self.layer2(x)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 122, in forward
    out = self.conv3(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 443, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 15.78 GiB total capacity; 3.30 GiB already allocated; 39.25 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 408, in <module>
    main()
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 254, in main
    scores, queue = train(train_loader, model, optimizer, epoch, lr_schedule, queue)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 312, in train
    embedding, output = model(inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 317, in forward
    _out = self.forward_backbone(torch.cat(inputs[start_idx: end_idx]).cuda(non_blocking=True))
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 284, in forward_backbone
    x = self.layer1(x)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 115, in forward
    out = self.bn1(out)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 2421, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 15.78 GiB total capacity; 2.15 GiB already allocated; 39.25 MiB free; 2.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 408, in <module>
    main()
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 254, in main
    scores, queue = train(train_loader, model, optimizer, epoch, lr_schedule, queue)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 312, in train
    embedding, output = model(inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 317, in forward
    _out = self.forward_backbone(torch.cat(inputs[start_idx: end_idx]).cuda(non_blocking=True))
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 285, in forward_backbone
    x = self.layer2(x)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 126, in forward
    identity = self.downsample(x)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 2421, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 15.78 GiB total capacity; 3.20 GiB already allocated; 87.25 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Traceback (most recent call last):
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 408, in <module>
    main()
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 254, in main
    scores, queue = train(train_loader, model, optimizer, epoch, lr_schedule, queue)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/swav_dp.py", line 312, in train
    embedding, output = model(inputs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/_utils.py", line 457, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 317, in forward
    _out = self.forward_backbone(torch.cat(inputs[start_idx: end_idx]).cuda(non_blocking=True))
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 284, in forward_backbone
    x = self.layer1(x)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/tz408/Semi-Supervised-Object-Detection/swav_dp/src/resnet50.py", line 126, in forward
    identity = self.downsample(x)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 2421, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 15.78 GiB total capacity; 1.53 GiB already allocated; 139.25 MiB free; 1.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 6618 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 6620 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 6621 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 6619) of binary: /ext3/miniconda3/bin/python
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/ext3/miniconda3/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ext3/miniconda3/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
swav_dp.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-04-29_14:53:23
  host      : b-21-2.c.hpc-slurm-9c75.internal
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 6619)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
